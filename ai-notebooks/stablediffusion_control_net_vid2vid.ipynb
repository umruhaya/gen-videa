{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7571eb22",
   "metadata": {},
   "source": [
    "# ControlNet With Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the necessary packages (globally)\n",
    "!pip install torch diffusers opencv-python transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94ad6a10-adf9-4652-91bf-017f31065bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_encoder/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d26d094f5034c03a7681b47d38cd93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, Image, Audio\n",
    "import time\n",
    "import cv2\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# load control net extension\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "\n",
    "# load stable diffusion v1-5 or any other custom checkpoint\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"emilianJR/CyberRealistic_V3\", controlnet=controlnet, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# speed up diffusion process with faster scheduler and memory optimization\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# remove following line if xformers is not installed\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e792e7-0ff6-4a73-a434-d1e2c9a5cf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed for generator (this is used to create deterministic outputs)\n",
    "generator = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "925b0c4c-2c12-44cd-bffe-6fff0658566e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIDEO_NAME = \"umer-vid-hd.mp4\"\n",
    "INTERVAL = 3\n",
    "PROMPT = \"Hyper-detailed, full-face centered closeup of a person as a charismatic male, adorned with neon sigils, set against a backdrop of rain-soaked neon billboards. The art style combines Artgerm's finesse with the iconic rainy, neon-lit scenes from Ghost in the Shell.\"\n",
    "INFERENCE_STEPS = 50\n",
    "OUTPUT_NAME = \"test1.mp4\"\n",
    "FPS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc79eb",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### OpenCV Video to Image\n",
    "\n",
    "This function creates a capture device to load a video and convert all the frames into base 64 encoded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbe177f1-0a72-484d-a075-77f2f611b8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_video(video_name):\n",
    "    video = cv2.VideoCapture(video_name)\n",
    "\n",
    "    base64Frames = []\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "    video.release()\n",
    "    print(len(base64Frames), \"frames read.\")\n",
    "    return base64Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512908c",
   "metadata": {},
   "source": [
    "### Base64 to Pil Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e0c33a1-1d8a-47c6-8f81-4195fafefdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def base64_to_image(base64_string):\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "\n",
    "    # Create a BytesIO object from the binary data\n",
    "    image_io = BytesIO(image_data)\n",
    "\n",
    "    # Use PIL to create an image object\n",
    "    image = Image.open(image_io)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4281b",
   "metadata": {},
   "source": [
    "### Canny Image For Control\n",
    "\n",
    "It uses an vision algorithm to detect edges and the result image is called canny image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0a023aa-e045-4920-9ef6-568a8995ba35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_canny_image(original_image):\n",
    "    image = np.array(original_image)\n",
    "    # get canny image\n",
    "    MIN_THRESHOLD = 100\n",
    "    MAX_THRESHOLD = 100\n",
    "    image = cv2.Canny(image, MIN_THRESHOLD, MAX_THRESHOLD)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    canny_image = Image.fromarray(image)\n",
    "    return canny_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e8e8a",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "208d15e4-3735-45e8-bf41-245852376876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_image(prompt, canny_image):\n",
    "    image = pipe(\n",
    "        prompt, num_inference_steps=INFERENCE_STEPS, generator=generator, image=canny_image\n",
    "    ).images[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10cf9b0a-bd33-4ba7-a881-7f653382a8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compile_video(output_name, frames, fps):\n",
    "    export_to_video(frames, output_name, fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768c9a4",
   "metadata": {},
   "source": [
    "## Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885e360-222a-463f-b907-6a86f0a1787c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load video to frames\n",
    "frames = load_video(VIDEO_NAME)\n",
    "\n",
    "# skip frames in between\n",
    "frames = frames[0::INTERVAL]\n",
    "print(len(frames), \"after trimming\")\n",
    "\n",
    "# convert base64 to PIL object array\n",
    "frames = [base64_to_image(frame) for frame in frames]\n",
    "\n",
    "# resize\n",
    "frames = [img.resize((512, 512)) for img in frames]\n",
    "\n",
    "output_images = []\n",
    "\n",
    "# iterate for each frame\n",
    "for i, frame in enumerate(frames):\n",
    "    print(f\"generating {i+1}/{len(frames)}\")\n",
    "    canny_image = generate_canny_image(frame)\n",
    "    image = generate_image(PROMPT, canny_image)\n",
    "    output_images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae362d6e",
   "metadata": {},
   "source": [
    "### Export a Video For Comparison\n",
    "\n",
    "Lets export the video such that we keep the original frame and ai generated image side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49a4e19f-f7bf-48cd-8e62-a490386bf168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def stack_images_side_by_side(image1, image2):\n",
    "    width1, height1 = image1.size\n",
    "    width2, height2 = image2.size\n",
    "\n",
    "    # Create a new image with a width equal to the sum of the original images' widths\n",
    "    # and a height equal to the maximum height of the two images\n",
    "    new_width = width1 + width2\n",
    "    new_height = max(height1, height2)\n",
    "\n",
    "    # Create a new image with the appropriate size and a white background\n",
    "    new_image = Image.new('RGB', (new_width, new_height), (255, 255, 255))\n",
    "\n",
    "    # Paste the first image into the new image\n",
    "    new_image.paste(image1, (0, 0))\n",
    "\n",
    "    # Paste the second image into the new image, immediately to the right of the first image\n",
    "    new_image.paste(image2, (width1, 0))\n",
    "\n",
    "    return new_image\n",
    "\n",
    "combined_frames = []\n",
    "for i in range(len(frames)):\n",
    "    stacked = stack_images_side_by_side(frames[i], output_images[i])\n",
    "    combined_frames.append(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73705489-187f-4419-b7f0-1d92b14c822e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compile_video(\"vid2vid-3-input.mp4\", frames, fps=4)\n",
    "compile_video(\"vid2vid-3-output.mp4\", output_images, fps=4)\n",
    "compile_video(\"vid2vid-3-combined.mp4\", combined_frames, fps=4)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
